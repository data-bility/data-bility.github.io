---
title: 주간 품절상품분류 모델 개발기
date: 2025-06-30 00:00:00 +09:00
categories: [experiments]
math: true
---

> - 본 글은 NE.O 자동화물류센터(NE.O.001, NE.O.003)에서 **주간 품절 위험 SKU를 조기 탐지**하기 위해 구축한 분류모델 개발 사례를 다룹니다.  
> - 초기 버전에서는 경험적으로 산정한 **고정 Cut-off (0.7)** 을 사용하였으며, 회고에서 다루겠지만 이후에 데이터 분포의 변동성으로 인해 **동적 Cut-off 로직**을 추가로 개발하게 되었습니다.  
> - 해당 내용은 별도 글 _「사후 평가 기반 절대·상대 혼합 Cut-off 로직 개발」_ 에서 다룹니다.

---

## 1. 배경과 목표

- **성과 요약 (과거 6개월 기간에 대한 주간 시뮬레이션)**
  - 시뮬레이션 과정에서 성능 수준을 확인하며 경험적인 **Cut-off = 0.7 (절대값 고정)** 설정 하에 :
    - Precision 평균 **0.65 (최대 0.79)**
    - Recall 평균 **0.40 (최대 0.67)**
    - F1-score 평균 **0.46 (최대 0.55**


- **문제 상황**  
  NE.O 자동화물류센터는 SKU별 재고 회전율과 입하 주기가 다르기 때문에, 품절 발생 시 빠른 대응을 위해 **주간 단위 품절 위험 상품(Positive SKU)** 을 선별할 필요가 있었습니다.


- **모델 목표**
  - 주간 기준, 한 번이라도 품절(Y)이 발생할 가능성이 높은 SKU 예측
  - 팀내 정책적으로 최소한 **Precision >= 0.5** 수준 유지하며 Positive SKU를 수백 개 단위(너무 적지도, 많지도 않는 수준)로 확보
    - 이러한 정책을 갖게 된 사유는, 분류 결과를 기존 수요예측값 상향에 이용하기 위함이었으며 이 내용은 추후 작성할 포스트에서 다룸. 

---

## 2. 파이프라인 개요

> - 주간 월요일 오전 기준으로 전체 프로세스가 실행됩니다.  
> - Spark 기반 배치 파이프라인, 총 6단계

```text
(1) 재고, 판매 기반 기초 지표 생성
(2) 리드타임 mode값, 예측수량 결합, train/forecast 세트 생성
(3) 추가 피처 생성 (spike, gradient, abst 경과일 등)
(4) LGBMClassifier + TimeSeriesSplit 기반 전체 SKU x 센터 글로벌 학습
(5) 다음 주 SKU별 품절확률 예측 + 절대값 cutoff 기준으로 분류
(6) 지난 주 실제 품절(Y/N) 참값 생성, 예측성능 모니터링 테이블 생성
```

---

## 3. 데이터 및 특성 설계

### 3.1 설계 원칙
- 리드타임(Lead-time) 구간을 기준으로 수요, 예측, 재고의 상대적 관계를 표준화하여, 품절 가능성을 설명할 수 있는 균형(Balance), 모멘텀(Momentum), 이력(History) 피처를 구성함.
- SKU x 센터별 최근 10건 발주의 Modal Lead-time 을 사용하여 운영 패턴을 반영.

### 3.2 주요 피처 요약


| 구분       | 변수                                                       | 설명                   |
|----------| -------------------------------------------------------- |----------------------|
| 수요/예측 요약 | $\overline{D}*{LT}$, $\overline{\hat D}*{LT}$, $cv_{LT}$ | 리드타임 widnow 평균, 변동계수 |
| 이동평균     | $\overline{D}*{30}$, $cv*{30}$                           | 30일 이동 통계            |
| 재고-수요 균형 | spike_score, days_of_supply_hat                          | 재고 대비 수요 비율          |
| 로그 안정화   | bal_log_hat                                              | 공급일수의 로그 스케일 안정화     |
| 모멘텀      | dos_grad_3d                                              | 3일간 수요/재고 기울기        |
| 이력       | days_since_last_abst                                     | 직전 품절 이후 경과일         |
| 기타       | evt_scr, week_scr, log_sellprc, dcrt, capa 등             | 프로모션/가격 요인           |


> #### 짧게 써보는 배운점
> - 안전재고의 개념적 정의를 녹여 '재고-수요 균형'에 해당하는 피쳐를 개발했는데, 시뮬레이션 기간 내 주차 별 feature importance를 확인해보면 대체로 가장 높은 값을 가졌습니다.
>   - 물론 이것은 해당 피쳐들이 노드 분할에 많이 사용되었다는 의미일 뿐이지만, 도메인 지식이 실무적인 특성공학에 도움이 되는 것은 사실이구나! 를 확인했습니다.  
> - 다른 피쳐보다 모멘텀 형태의 피쳐가 Precision 성능 향상에 가장 효과를 크게 보였습니다.


---


## 4. 모델링

### 4.1 학습 방식
- 모델 : LightGBMClassifier(objective='binary')
- 검증 : TimeSeriesSplit(n_splits=4, gap=7), (7일 간격으로 시계열 누적 검증)
- 데이터 불균형 보정 : scale_pos_weight = k * (neg / pos) (초기 실험은 k=1, 2, ... ,6 등으로 진행했으나 성능 저하가 발견되어 1로 고정하여 사용)
- 입력 컬럼 구성
  - 식별자(str_no, sku_id 등) -> 해시 인코딩 
  - 범주형(grp, mode_leadtm_rcnt10, leadtm_cyc) -> 원 핫 인코딩 
  - 수치형 -> 그대로 사용

```text
for k in {1.0}:
  for each TimeSeriesSplit fold:
    preproc = ColumnTransformer([id_hash, OneHotEncoder, passthrough])
    LGBMClassifier(scale_pos_weight=k*neg/pos)
    train → validate (metric=AUC)
  select best fold by AUC → save Pipeline(pickle to S3)

```

### 4.2 데이터 불균형 보정 방식에 대한 실험

<img src="/assets/img/scale_pos_weight_experiments.png" width="500px" height="350px" alt="scale_pos_weight 실험 결과">

- **불균형 보정** : `scale_pos_weight = k * (neg / pos)`
  - Positive(품절) 비율이 낮은 데이터의 불균형을 보정하기 위해, `k` 값을 1~6 사이로 변화시키며 실험을 진행함.
  - 그래프에서 보이듯, **k=1**일 때 주간별 F1-score가 전반적으로 가장 우수했음.
  - 이는 모델이 이미 `class_weight` 조정 없이도 충분히 안정적으로 학습되고 있음을 의미하며, 오히려 높은 `k`값은 Recall 증가에 비해 Precision 저하가 더 커지는 경향을 보였음.
  - 결과적으로 k=1로 고정하여 사용함.

---


## 5. 예측 및 지난 주 참값 생성
- 예측 단계
  - 학습된 모델을 이용하여 SKU별 품절 확률 $ p $ 산출 
  - SKU 단위 최대 확률(max(prob))을 대표값으로 사용 
  - 이후 Cut-off=0.7 을 기준으로 주간품절상품분류 = Y/N 분류


- 참값 생성(성능 모니터링)
  - 지난 주(월~일) 기간 중 하루라도 품절 발생 시 주간품절상품분류='Y'

```text
# 예측
weekly_prob = groupby(SKU).max(model.predict_proba(features)[:,1])
weekly_abst_clsf = (prob >= 0.7) ? 'Y' : 'N'

# 참값
weekly_abst_yn = max(daily_abst_yn in last_week)
```


---


## 6. 시뮬레이션 결과 (테스트 시점으로부터 과거 6개월 소급, 성능평가)

- Precision 평균 **0.65 (최대 0.79)**
- Recall 평균 **0.40 (최대 0.67)**
- F1-score 평균 **0.46 (최대 0.55**

> - Positive SKU 확보 수가 일정 수준 이상 유지되며, 주차별 Precision 변동 폭은 $ \pm $ 0.1 이내로 안정적이었음. <br>
> - Cut-off 0.7 기준으로 불필요한 과잉 탐지(False Positive) 를 방지하면서 운영 개입 리소스(작업 SKU 수) 를 현실적으로 통제 가능하다고 생각했음.
>   - 회고에서 다루겠지만, 대규모 행사 시에 데이터에 변화가 발생하며 이러한 절대값 방식에 결함이 있음을 확인했음.


---


## 7. 회고 및 한계

우선, 개인적인 감상이지만 이러한 구조의 학습/분류 형태를 만들어보면서 본인 스스로 주간 학습 -> 일간 예측하는 구조에 너무 길들여진 게 아닐까 하는 생각이 들었습니다. <br>

그 외에 분류 모델 자체의 한계점을 다뤄보면 다음과 같습니다. : 

- 한계 1 : 경험적 고정 Cut-off(0.7)는 시뮬레이션 기간에는 유효했으나, 이후 데이터 분포가 상향 이동하며 **확률값의 전반적 상승(데이터 드리프트)** 이 관찰됨.
- 한계 2 : 주차별 품절 빈도 변화, 이벤트 영향 등 외생요인 반영이 어려움.
- 한계 3 : Positive 비율이 낮은 구간에서는 Recall 변동성이 커짐.

특히, 한계 1은 분류 성능에 대한 문제가 발견된 것이라 조속히 개선해야할 것 입니다. 모델 자체의 강건성을 끌어올리는 방법을 찾을 수 있다면 좋겠지만, 학습 가능한 데이터의 양이 제한적이고, 운영 반영까지의 마감 기한도 존재하므로 휴리스틱한 방법을 동원해서라도 정책 상 최소 정확도를 유지할 수 있도록 추가 로직을 개발해야할 것 같습니다.

팀 내에서 합의된 목표가 다음과 같음을 고려하였을 때 :
1) 이후 분류 결과를 수요예측 모델에 사용해야하므로, 수백 개 수준의 양성 분류를 하게끔 모델링한다.
2) 실제 성능을 Precision 기준으로 판단하되, 최소 0.5를 유지한다.

짧은 기간 내 가능할 것으로 보이는 개선안은 다음과 같습니다 :
> 워크 포워드 방식으로 매주 성능 평가를 하여 절대/상대 혼합 cut-off를 센터 별로 재계산한다.

이러한 안을 생각해낸 까닭은, 우선 두 센터를 묶어서 함께 학습하는 구조이고 두 센터의 데이터가 갖는 특성에 차이가 있다는 것을 확인했기 때문입니다. <br>
개선이 가능한지 진행해보고, 후속 글을 작성하도록 하겠습니다.
